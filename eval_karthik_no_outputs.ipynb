{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6764f42",
   "metadata": {},
   "source": [
    "# Evaluation Exercise for \"Machine Learning in Julia for Calorimeter Showers\"\n",
    "\n",
    "Submission by N. V. Karthik (https://github.com/NV-Karthik)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096b192f",
   "metadata": {},
   "source": [
    "## 1. Preliminary Checks and Imports\n",
    "\n",
    "Checking the environment to ensure correct Project.toml/Manifest.toml files are being tied to the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244516f-3b1d-4ce6-b961-6dcff131657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the virtual environment the notebook is running\n",
    "println(Base.active_project())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fcb199",
   "metadata": {},
   "source": [
    "Here to enable GPU compute I am using CUDA (which now comes with CuArrays and cuDNN packages inbuilt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17a6497-03db-45c1-aa95-ad5656bea047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to import datasets\n",
    "using CSV  \n",
    "using DataFrames\n",
    "\n",
    "# train neural networks\n",
    "using Flux  \n",
    "using CUDA   # enable GPU compute\n",
    "using Statistics  # get statistics\n",
    "\n",
    "# miscellaneous\n",
    "using Random, ProgressMeter  \n",
    "\n",
    "# for results and accuracy-scores\n",
    "using Plots\n",
    "using EvalMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53609727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check GPU backend\n",
    "\n",
    "println(\"is GPU functional -\", CUDA.functional())\n",
    "println(\"GPU Backend -\",Flux.GPU_BACKEND)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9729fb",
   "metadata": {},
   "source": [
    "## Function definitions\n",
    "\n",
    "1. ```trainTestSplit```:Custom function is written to perform train-test split directly on dataframes, solely to avoid unnecessary juggling between datatypes that is demanded by some utility-function.\n",
    "\n",
    "- It makes use of ```Random.shuffle``` method to randomly split it according to the ratio\n",
    "- It returns <b>copies</b> of dataframe <b>views</b> which can be treated as seperate dataframes\n",
    "\n",
    "2. ```normalizeData``` to normalize datasets\n",
    "\n",
    "3. ```printAccuracies```: prints Confusion Matrix, Balanced Accuracy, F1 score, Precision, Recall, Accuracy given target and prediction arrays\n",
    "\n",
    "4. ```plotConfusionMatrix```: plots confusion matrix given target and prediction arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c9409d-2655-4498-98e7-b10c15073147",
   "metadata": {},
   "outputs": [],
   "source": [
    "function trainTestSplit(df, train_ratio)\n",
    "    # check if train_ratio is out of bounds\n",
    "    @assert 0 <= train_ratio <= 1 \"train ratio should be between 0 and 1\"\n",
    "    \n",
    "    # get row indices and shuffle them in-place\n",
    "    indices = collect(axes(df, 1)) #\n",
    "    shuffle!(indices)\n",
    "    \n",
    "    train_ids = indices .<= (nrow(df) .* train_ratio)\n",
    "    \n",
    "    # create train and test dataframes\n",
    "    train_df = copy(view(df, train_ids, :))\n",
    "    test_df = copy(view(df, .!train_ids, :))\n",
    "    \n",
    "    return train_df, test_df \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a8aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "function normalizeData(X)\n",
    "    mu = mean(X,dims=1)\n",
    "    sigma = std(X,dims=1)\n",
    "    \n",
    "    X_norm = (X .- mu) ./ sigma\n",
    "    \n",
    "    return X_norm\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dd8df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "function printAccuracies(actuals, predicts)\n",
    "\n",
    "    # Calculate balanced accuracy\n",
    "    bal_accuracy = balanced_accuracy(actuals, predicts)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(actuals, predicts)\n",
    "\n",
    "    # Calculate precision\n",
    "    precision_score = precision(actuals, predicts)\n",
    "\n",
    "    # Calculate recall\n",
    "    recall_score = recall(actuals, predicts)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy_score = accuracy(actuals, predicts)\n",
    "\n",
    "    # Print the results\n",
    "    println(\"Balanced Accuracy: \", round(bal_accuracy, digits=3))\n",
    "    println(\"F1 Score: \", round(f1, digits=3))\n",
    "    println(\"Precision: \", round(precision_score, digits=3))\n",
    "    println(\"Recall: \", round(recall_score, digits=3))\n",
    "    println(\"Accuracy: \", round(accuracy_score, digits=3))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2418cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "function plotConfusionMatrix(actuals, predicts)\n",
    "    # Compute confusion matrix\n",
    "    cm = EvalMetrics.ConfusionMatrix(actuals, predicts)\n",
    "    matrix_data = [cm.tp cm.fp ; \n",
    "                   cm.fn cm.tn]\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    heatmap(matrix_data, aspect_ratio=:equal, xlabel=\"True Label\", ylabel=\"Predicted Label\", \n",
    "        xticks=([1, 2], [\"s\", \"b\"]), yticks=([1, 2], [\"s\", \"b\"]), color=:viridis, title=\"Confusion Matrix\", \n",
    "        titlefontsize=12, grid=false, yflip=true)\n",
    "\n",
    "    # Annotate each square with the corresponding count\n",
    "    for (i, true_label) in enumerate(0:1)\n",
    "        for (j, pred_label) in enumerate(0:1)\n",
    "            count = matrix_data[i, j]\n",
    "            annotate!([(j, i, text(string(count), 12, :white))])\n",
    "        end\n",
    "    end\n",
    "\n",
    "    plot!()\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a41bc4",
   "metadata": {},
   "source": [
    "## 2. Importing Datasets and Statistical overview\n",
    "\n",
    "- ```CSV``` is used to import dataset as a dataframe, which is then split into train-test sets\n",
    "- validation set can also be extracted using the above function, but it is not in the scope of the assignment. Test is also auxillary considering that the assignment requires accuracy over the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44fb097-3e7f-405f-b2aa-b875a7dddc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract dataset\n",
    "data = CSV.read(\"./dataset.csv\", DataFrame, header=1, delim=\",\")\n",
    "\n",
    "# check dataset for any NaNs\n",
    "nan_values = any(ismissing.(eachcol(data)))\n",
    "\n",
    "if nan_values\n",
    "    println(\"DataFrame contains NaN values.\")\n",
    "else\n",
    "    println(\"DataFrame does not contain NaN values.\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e55b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "train, test = trainTestSplit(data, 0.7)\n",
    "\n",
    "# seperating feature arrays and target arrays\n",
    "Xtrain = permutedims(Matrix(train[:, 1:3]));\n",
    "ytrain = Array(train[:, 4]);\n",
    "Xtest = permutedims(Matrix(test[:, 1:3]));\n",
    "ytest = Array(test[:, 4]);\n",
    "\n",
    "# feature and target separation for whole dataset\n",
    "X = permutedims(Matrix(data[:, 1:3]));\n",
    "y = Array(data[:, 4]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d650cdc",
   "metadata": {},
   "source": [
    "<b>From the below data we can see that the data set is looks normalised but is skewed. So no further normalization is required</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check mean, variance and normalization of dataset\n",
    "\n",
    "pos_x = Array(data[:, 1])\n",
    "pos_y = Array(data[:, 2])\n",
    "pos_z = Array(data[:, 3])\n",
    "\n",
    "println(\"For pos_x: Mean:$(mean(pos_x)), Standard Deviation:$(std(pos_x)), Min:$(minimum(pos_x)), Max:$(maximum(pos_x))\")\n",
    "println(\"For pos_y: Mean:$(mean(pos_y)), Standard Deviation:$(std(pos_y)), Min:$(minimum(pos_y)), Max:$(maximum(pos_y))\")\n",
    "println(\"For pos_z: Mean:$(mean(pos_z)), Standard Deviation:$(std(pos_z)), Min:$(minimum(pos_z)), Max:$(maximum(pos_z))\")\n",
    "println(\"For class: Number of signals: $(sum(y .== \"s\")), Number of backgrounds: $(100000-sum(y .== \"s\"))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cab398",
   "metadata": {},
   "source": [
    "<b>uncomment the cell below and run it to visualise the dataset in 3d. But also note that doing so can slow down the jupyter notebook</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f782ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize dataset\n",
    "\n",
    "# plot3d = Plots.plot(pos_x,pos_y, pos_z, seriestype=:scatter, markersize = 1, group=y, legend=true)\n",
    "# display(plot3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9f6153",
   "metadata": {},
   "source": [
    "## 3. Defining and Training the Neural Network Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11ef185",
   "metadata": {},
   "source": [
    "### Model Parameter/Hyper Parameter Choices\n",
    "\n",
    "- The simplest neural network which I started with gave good enough accuracies with the test-set and whole data set. So I refrained to add further complications considering the higher compute power required for little more accuracy\n",
    "- The Input ```Dense``` layer consists of 3 neurons for the 3 features in dataset, which moves into 10 units in the hidden layer. The final layer has 2 units each for the each desired output.\n",
    "- Since the feature array size is low and the features are independent of each other (x, y, z coordinates), the simplest network must be chosen to avoid overfitting.\n",
    "- Thus 1 hidden layer with sigmoid activation is chosen as sigmoid the most used activation for logistic regression\n",
    "- <b>Sigmoid</b> activation is used for output layer as it is preferred for binary classification and its advantages of giving results open to probabilistic interpretation. Softmax can be used for multiclass classification.\n",
    "- <b>Binary Cross-Entropy</b> loss function is preferred with sigmoid for binary classification, thus making its choice obvious. \n",
    "- <b>Adam</b> Optimizer is used with a moderate 0.01 learning rate due to its popularity with neural networks. Learning momenta are left to their default values\n",
    "\n",
    " \n",
    "<br> </br>\n",
    "Note: Additional <b>Batch Normalization</b> layer with <b>relu</b> activation is also tried on the network for two reasons,\n",
    "1. as the data is not normalized \n",
    "2. and due to its proven effects on accuracy and training speed.   \n",
    "Relu is chosen due as it can introduce a subtle non-linear activation which is required here.\n",
    "\n",
    "And defying expectations Batch normalised data performs slightly worse than the non-Batch normalised model on the test set. (95% vs 98% accuracy).  \n",
    "Also Batch-normed model also takes significantly more time to train (~25 min for non Batch normalised vs ~ 35min for Batch normalised) and raises errors with cuDNN to evaluate larger input arrays (eg. test set with 30000 rows will work but the whole dataset will overflow gpu memory),\n",
    "Hence a model without batch normalization is used here.\n",
    "\n",
    "Nevertheless batch normed model can be tried by uncommenting the ```BatchNorm(10, relu),``` line below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70393849-3847-4752-8e39-86cc990cbab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "\n",
    "model = Chain(\n",
    "    Dense(3=>10, σ, bias=true),  # Input layer with 3 neurons, hidden layer with 64 neurons\n",
    "#     BatchNorm(10, relu),\n",
    "    Dense(10=>2),      # Output layer with 2 neurons (for binary classification)\n",
    "    σ            # sigma activation function for binary classification\n",
    "    ) |> gpu  # Move model to gpu if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7243f2-a6b8-45fd-ac4d-a87324fc7fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "\n",
    "loss_func = Flux.binarycrossentropy\n",
    "optim = Flux.setup(Flux.Adam(0.01), model);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d5e1a1",
   "metadata": {},
   "source": [
    "## Dataloading\n",
    "- target arrays are one-hot encoded for performance reasons\n",
    "- batch size of input features is taken as 64 not too big or not too small, to account for both memory and statistical sampling constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6c2423-4691-416f-a15b-aae18de2256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "ytrain_onehot = Flux.onehotbatch(ytrain, [\"s\", \"b\"]);\n",
    "loader = Flux.DataLoader((Xtrain, ytrain_onehot) |> gpu, batchsize=64, shuffle=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fabcb93",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Generic training loop is presented with 1000 epochs (which was decided from the loss vs epochs plot shown further below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032725ac-5d04-4e8f-b607-689ce1db92b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop, using the whole data set 1000 times:\n",
    "losses = []\n",
    "@showprogress for epoch in 1:1_000\n",
    "    for (x, y) in loader\n",
    "        loss, grads = Flux.withgradient(model) do m\n",
    "            # Evaluate model and loss inside gradient context:\n",
    "            y_hat = m(x)\n",
    "            loss_func(y_hat, y)\n",
    "        end\n",
    "        Flux.update!(optim, model, grads[1])\n",
    "        push!(losses, loss)  # logging, outside gradient context\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255ba807",
   "metadata": {},
   "source": [
    "# 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5005f394",
   "metadata": {},
   "source": [
    "Firstly predictions on the test set are evaluated, and its accuracy is obtained to be <b>98.6%</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd941d7-9e22-4fec-8cca-0f280cc00acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions on the test set\n",
    "\n",
    "y_pred = model(Xtest |> gpu) |> cpu\n",
    "\n",
    "# getting the test set accuracy\n",
    "\n",
    "y_actual = Array(ytest .== \"s\")\n",
    "test_predicts = Array(y_pred[1,:] .> 0.5)\n",
    "test_acc = mean( test_predicts .== y_actual) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9c6266",
   "metadata": {},
   "source": [
    "Then predictions on the whole dataset is evaluated, its accuracy can be seen as <b>96.8%</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beda8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy over the whole dataset\n",
    "\n",
    "y_pred_total = model(X |> gpu) |> cpu\n",
    "\n",
    "y_actual_total = Array(y .== \"s\")\n",
    "total_predicts = (y_pred_total[1,:] .> 0.5)\n",
    "\n",
    "total_acc = mean(total_predicts .== y_actual_total) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cd9d58",
   "metadata": {},
   "source": [
    "## More Evaluation Metrics on the Test set\n",
    "\n",
    "model's\n",
    "\n",
    "- Confusion Matrix\n",
    "- Balanced Accuracy (necessary for skewed datasets like this one)\n",
    "- F1 score\n",
    "- Precision\n",
    "- Recall\n",
    "- Accuracy\n",
    "\n",
    "are included as a model's standard performance metrics  \n",
    "\n",
    "\n",
    "(from stdout)  \n",
    "Balanced Accuracy: 0.982  \n",
    "F1 Score: 0.97  \n",
    "Precision: 0.966  \n",
    "Recall: 0.974  \n",
    "Accuracy: 0.986"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80abaabd-035b-43d8-8d68-10f8a64fecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotConfusionMatrix(y_actual, test_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae416c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "printAccuracies(y_actual, test_predicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2246ae8",
   "metadata": {},
   "source": [
    "## More Evaluation Metrics of model on the Whole dataset\n",
    "\n",
    "same metrics are presented here as well  \n",
    "\n",
    "\n",
    "(from stdout)  \n",
    "Balanced Accuracy: 0.98  \n",
    "F1 Score: 0.967  \n",
    "Precision: 0.963  \n",
    "Recall: 0.971  \n",
    "Accuracy: 0.985\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6cd9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotConfusionMatrix(y_actual_total, total_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fdc703",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "printAccuracies(y_actual_total, total_predicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249f64cf",
   "metadata": {},
   "source": [
    "## Loss variation as Training progresses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04d4c12",
   "metadata": {},
   "source": [
    "<b>uncomment the cell below and run it to visualise the dataset in 3d. But also note that doing so can slow down the jupyter notebook</b>  \n",
    "\n",
    "or you can refer to the loss_curve.png for the same figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf76ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot(losses; xaxis=(:log10, \"iteration\"),\n",
    "#     yaxis=\"loss\", label=\"per batch\")\n",
    "# n = length(loader)\n",
    "# plot!(n:n:length(losses), mean.(Iterators.partition(losses, n)),\n",
    "#     label=\"epoch mean\")\n",
    "# # savefig(\"loss_curve.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aafeaae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
